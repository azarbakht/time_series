---
title: "Time Series"
author: "Emerson Amirhosein  Azarbakht"
output: pdf_document
---

# Time Series (TS)

#### Used in
- control of inventory, based on demand trends
- airline's decision to buy airplanes bc of passenger trends and decision to increase/maintain market share
- climate change decisions based on temperature change trends
- business/sales forecasting
- everyday operational decisions
- long-term effects of proposed water management policies by simulating daily rainfall and sea state time series
- understanding fluctuations in monthly sales
- basis for signal processing in telecommunications <?>
- disease incidence tracking, yearly rates
- census analysis
- tracking monthly unemployment rate; as an economic indicator used by decision makers

#### Used to
- to understand the past, and predict the future
- forcasting (predicting inference, a subset of statistical inference). assumes that present trends continue. This assumption cannot be checked empirically, but, when we identify the likely causes for a trend, we can justify the forecasting(extrapolating it) for a few time-steps at least
- anomaly detection
- clustering
- classification (assigning a time series pattern to a specific category: e.g. gesture recognition of hand movements in sign language videos)
- query by content ~ Content-based image retrieval

#### Data: 
a variable measured sequentially in time, or at a fixed [sampling] interval 

#### serial dependence problem:
observations close together in time tend to be correlated (serially dependent)

TS tries to explain this correlation (serial dependence)
autocorrelation analysis examines this serial dependence <?>

### conditions (assumptions of TS)
- Stationary process:
  * if there's no systematic change in mean (no trend) AND
  * if there's no systematic change in variance, AND
  * if strictly periodic variations have been removed
    * i.e. the properties of one section of the data are much like those of any other section
    * often we have a non-stationary TS => we need to remove trend and seasonality, to get a stationary residual, which then can be modeled using a stationary stochastic process 
- Ergodic process: a stationary TS that we assume is sufficiently long time series that it characterises the hypothetical model. (~ independent of the starting point)

```{r}
plot(AirPassengers)
start(AirPassengers)
end(AirPassengers)
frequency(AirPassengers)
plot(AirPassengers)
summary(AirPassengers)
layout(1:2)
# takes an input matrix for the location of each plot in the graphics window
plot(aggregate(AirPassengers))
boxplot(AirPassengers ~ cycle(AirPassengers))
```

plotting shows _patterns_, and _features_ of the data + *outliers* and *erroneous* values 


### patterns
1. trend = a non-periodic systematic change in a TS: a long-term change in the 'mean'
    * can be modeled simply by a linear increase or decrease. (only if it's deterministic ~ it's non-stochastic)
    * stochastic trend: seems to change direction at unpredictable times rather than displaying a consistent pattern (e.g. like the air passenger series); inexplicable changes in direction
2. seasonal variation = a repeating pattern within a fixed period (e.g. each year)
3. cycles = a non-fixed-period cycle (without a fixed period). example: El-Nino

![El Nino picture](El-Nino.gif)

##### Monthly unemployment rate for a state
```{r}
# monthly unemployment rate for the US state of Maine from January 1996 until August 2006
Maine.month <- read.table("http://staff.elena.aut.ac.nz/Paul-Cowpertwait/ts/Maine.dat", header = TRUE)
# header TRUE means treat first row as column names
attach(Maine.month)
str(Maine.month)
head(Maine.month)
class(Maine.month)
# it's a data.frame, not a ts object. So, we need to convert it to ts
Maine.month.ts <- ts(unemploy, start = c(1996, 1), freq = 12)
Maine.month.ts
Maine.annual.ts <- aggregate(Maine.month.ts)/12
layout(1:2)
plot(Maine.month.ts)
plot(Maine.annual.ts)
Maine.Feb <- window(Maine.month.ts, start = c(1996,2), freq = TRUE)
Maine.Aug <- window(Maine.month.ts, start = c(1996,8), freq = TRUE)
Feb.ratio <- mean(Maine.Feb) / mean(Maine.month.ts)
Aug.ratio <- mean(Maine.Aug) / mean(Maine.month.ts)
```
---
#### Natoionwide unemployment rate
```{r}
# monthly unemployment rate for all of the United States from January 1996 until October 2006
US.month <- read.table("http://staff.elena.aut.ac.nz/Paul-Cowpertwait/ts/USunemp.dat", header = T)
attach(US.month)
US.month.ts <- ts(USun, start=c(1996,1), end=c(2006,10), freq = 12)
plot(US.month.ts, ylab = "unemployed (%)")
```
---
### Multiple TS 
```{r}
ChocolateBeerElectricity <- read.table("http://staff.elena.aut.ac.nz/Paul-Cowpertwait/ts/cbe.dat", header = TRUE)
class(ChocolateBeerElectricity)
str(ChocolateBeerElectricity)
Chocolate.ts <- ts(ChocolateBeerElectricity[,1], start = 1958, frequency = 12)
Beer.ts <- ts(ChocolateBeerElectricity[,2], start = 1958, frequency = 12)
Electricity.ts <- ts(ChocolateBeerElectricity[,3], start = 1958, frequency = 12)

plot(cbind(Chocolate.ts, Beer.ts, Electricity.ts))

# intersection of multiple TS
compareTwoTSChocolateBeer <- ts.intersect(Chocolate.ts, Beer.ts)
# bind time series which have a common frequency
?ts.intersect()
head(compareTwoTSChocolateBeer)

layout(1:2)
plot(compareTwoTSChocolateBeer[,1])
plot(compareTwoTSChocolateBeer[,2])
cor(Chocolate.ts, Beer.ts)
# correlation
```

#### Climate change
```{r}
# Global temperature anomalies from the monthly means over the period
GlobalTemperatures <- scan("http://staff.elena.aut.ac.nz/Paul-Cowpertwait/ts/global.dat")
# scan to read data into a vector or list from the console or file.
?scan
str(GlobalTemperatures)
GlobalTemperatures.ts <- ts(GlobalTemperatures, st = c(1856,1), end = c(2005,1), frequency = 12)
head(GlobalTemperatures.ts)
tail(GlobalTemperatures.ts)
Global.annual <- aggregate(GlobalTemperatures.ts, FUN = mean)
head(Global.annual)
Global.annual
plot(GlobalTemperatures.ts)
plot(Global.annual)  

#
GlobalTemperatures1970to2005 <- window(GlobalTemperatures.ts, start=c(1970, 1), end=c(2005, 12))
# subset a time window
?window
GlobalTemperatures1970to2005.time <- time(GlobalTemperatures1970to2005)
# create the vector of times at which a time series was sampled
?time
plot(GlobalTemperatures1970to2005)
abline(reg = lm(GlobalTemperatures1970to2005 ~ GlobalTemperatures1970to2005.time))
```

--- 

In statistics, usually, first thing, we compute the 'mean' and 'variance/standard deviation', which show 'location' and 'dispersion':

- mean ~ location
- variance ~ dispersion

or

- mean ~ central location
- variance ~ the spread

### Transformations

Transform the data (e.g. log or square-root) to stabilize the variance, when:

1. if variance appears to increase with the mean.
    * if standard deviation is directly proportional to the mean (a trend), LOG transform!
    * if variance changes through time, but without a trend, transformation won't help. use a model that accommodates variance change
2. if there's additive seasonal effect, i.e. size of seasonal effect increases with the mean, transform! to make seasonal effect constant. 
    * If there's multiplicative seasonal effect, i.e. size of seasonal effect proportional to the mean, LOG transform! to make the seasonal effect additive. (variance gets stable, but error term will still remain unstable)
3. if there are spikes in the data, (skewness), transform to normalize the data distribution.

> avoid transformations, except where the transformed has a direct physical interpretation

### Sample autocorrelation coefficients

- a series of quantities, that measure the correlation between observations at different distances apart
- show us which probability model to use for that data


- negative correlation? shows high values of x tend to go with low values of y
- zero correlation? shows two variables are independent

#### serial correlation coefficients (e.g. at lag 1), 
or autocorrelation coefficients

- measures correlation between successive observations
- serial correlation coefficients at lag k
- if you graph it, it's called 'correlogram'
- ac.f and correlogram is meaningful ONLY IF data is STATIONARY
- ac.f and correlogram is meaningless for non-stationary

### how to interpret a Correlogram

- if r_0 = 1, r_1 = large value, r_2, r_3 = diminishingly successively smaller values, r_k = almost zero
    then, TS: one observation 'above' the mean tends to be followed by more observations 'above' the mean, and
              one observation 'below' the mean tends to be followed by more observations 'below' the mean

- if r_0 = 1, r_1 = negative value, r_2 = positive value, r_3 = negative, ... (diminishingly successively smaller values)
    then, TS: one observation 'above' the mean tends to be followed by more observations 'below' the mean,
            aka alternating: successive observations on both sides of the overall mean

- if r_0...k are all positive and large values, TS is non-stationary, and correlogram is meaningless

- if r_0...k oscillate, TS is 'seasonal', e.g. sinusoidal. check at least 3 seasons worth of r_k

### Covariance

$$ Cov(x,y) = E(xy) - E(x)E(y)$$
$$Var(x+y) = Var(x) + Var(y) + 2 Cov(x,y)$$

## Basic stochastic models

- residual error series =     observed data - fitted values (from the model)
- if our model is good (i.e. captures the deterministic features of the TS)
  * then residual TS should be a realization of independent random variables from a probability distribution

### White Noise (WN) [stochastic model for modeling residual TS]

- a TS is discrete WN, if the observations (variables) $w_1, ..., w_t$ are I.I.D. with mean = zero, and all variables have same variance $\sigma^2$ and their pairwise Covariance is zero. 
  * if, additionally, they are normally distributed, then it's Gaussian WN.
  


- mean = 0
- $Cov(w_i, w_j) = 0 for all i \neq j$
  
#### Simulation: to make a synthetic TS (vs. observed TS)


Why simulate?

- generate plausible future scenarios
- bootstrapping: constructing confidence intervals for the model parameters


```{r}
# provide a starting point for the random generation function, to make sure the random generation can be reproduced next time you're running the code, to get the same data
set.seed(1)

# rnorm simulates 100 random variables that are standard normal and independent
# which can be used as a Gaussian WN TS of length 100
w <- rnorm(100)

plot(w, type = "l")

# ?dnorm
# w2 <- dnorm(w, mean = 0, sd = 1, log = FALSE)
# plot(w2, type = "l")


x <- seq(-3,3, length = 1000)
hist(rnorm(100), prob = T)
points(x, dnorm(x), type = "l")

# correlogram of WN TS
set.seed(200)
acf(rnorm(200))
# the correlations (for lag > 1) is (almost because of sampling variation) zero for almost all (95%)
```

### Random Walk (RW) TS [stochastic model for modeling residual TS]

is a good model to fit to data with stochastic trends (not as good as ARIMA though)

TS ${x_t}$ is a RW, if $$x_t = x_{t-1} + w_t$$, where $w_t$ is a WN TS. So, we can back substitute $x_{t-1}$ with $x_{t-2} + w_{t-1}$ and so on, and get: $$x_t = w_1 + w_2 + w_3 + ... + w_t$$ (i.e. a finite sum of WN, each with mean = zero and var = $\sigma^2$ )

- mean = 0
- $Cov(x_t,x_{t+k}) = t\sigma^2$ => TS is non-stationary because the Covariance depends on time t. variance increases as t increases => RW model is good ONLY for short-term predictions

#### backshift(lag) operator (B)

$B x_t = x_{t-1}$
$B^n x_t = x_{t-n}$

#### difference operator (to make a stationary TS from a non-stationary TS)

$$\Delta x_t = x_t - x_{t-1}$$

$$\Delta x_t = (1 - B) x_t$$

$$\Delta^n = (1 - B)^n$$

```{r}
# generate WN TS as w
x <- w <- rnorm(1000)
# make a RW TS using backshift and WN TS
for (t in 2:1000) x[t] <- x[t - 1] + w[t]
# plot the RW TS with lines
plot(x, type = "l")
# draw the RW TS correlogram
acf(x)
```

---

#### diagnosis of a RW TS:

its correlogram should look like this:
```{r}
?diff
# this correlogram shows a RW TS (as a diagnostic tool)
acf(diff(x))
```

---

### Random Walk with drift TS [stochastic model for modeling residual TS]

e.g. for stocks, stockholders expect the value of their investment to increase despite the volatility of financial markets. (this increase can be mapped by a drift (upwards) parameter (delta))

$$x_t = x_{t-1} + \delta + w_t$$

```{r}
# HP stock prices
HP.dat <- read.table("http://staff.elena.aut.ac.nz/Paul-Cowpertwait/ts/HP.txt", header = T)
attach(HP.dat)
# plot the stock price TS
plot (as.ts(HP.dat$Price))
# calculate drift parameter \delta
drift <- diff(Price)
plot (as.ts(drift))
# check correlogram of drift to make sure it's a WN TS 
acf(drift)
# calculate confidence interval
mean(drift) + c(-2, 2) * sd(drift)/sqrt(length(drift))
```

### AutoRegressive TS of order p AR(p)[stochastic model for modeling residual TS]

when the model is a regression of $x_t$ on its past terms from the same TS.

$$x_t = \alpha_1 x_{t-1} + \alpha_2 x_{t-2} + ... + \alpha_p x_{t-p} + w_t$$ 

where

- $w_t$ is WN TS
- $\alpha_i$ are model parameters
- $\alpha_p \neq 0$ for an AR(p) TS

i.e. value of TS at time t depends not only on t-1 but t-2 and ... t-p (i.e. p steps back)

$$\theta(B)x_t = (1 - \alpha_1 B^1 - \alpha_2 B^2 - ... - \alpha_p B^p)x_t = w_t$$

#### how to tell if an AR(p) TS is stationary or non-stationary?

$\theta(B) = 1 - \alpha_1 B^1 - \alpha_2 B^2 - ... - \alpha_p B^p$ is called characteristic equation (CE). 

- If all roots of CE for an AR(p) are > 1, (their absolute value is > 1), AR(p) is stationary. 
- If even one of the roots of CE is <= 1 in absolute value, then AR(p) is non-stationary.

```{r}
# find roots of a polynomial
polyroot(c(1, 2, 1))

rho <- function(k, alpha) alpha^k
layout(1:2)
plot(0:10, rho(0:10, 0.7), type = "b")
plot(0:10, rho(0:10, -0.7), type = "b")

set.seed(1)
x <- w <- rnorm(100)
for (t in 2:100) x[t] <- 0.7 * x[t - 1] + w[t]
plot(x, type = "l")
acf(x)
pacf(x)
```

#### AR(p) TS model fitting in R

```{r}
# fit an AutoRegression AR(p) model
x.ar <- ar(x, method = "mle")

x.ar$order

# parameter estimate (alpha bar)
x.ar$ar
x.ar$asy.var
x.ar$ar + c(-2, 2) * sqrt(x.ar$asy.var)

# str(x.ar)
```

---
# Stationary Models

#### Strictly stationary TS

A time series model ${x_t}$ is strictly stationary if the joint statistical distribution of $x_{t1}, ..., x_{tn}$ is the same as the joint distribution of $x_{t1+m}, ..., x_{t_{n+m}}$ for
all $t_1, ..., t_n$ and m, so that the distribution is unchanged after an arbitrary
time shift.

- mean and variance are constant in time
- $Cov(x_t, x_s)$ only depends on the lag $k = |t - s|$

## Moving Average MA(q) TS model

MA(q) is a linear combination of current WN and the most recent past WN terms:

$$ x_t = w_t + \beta_1 w_{t-1} + ... + \beta_q w_{t - q} $$

where ${w_t}$ is a WN TS with 

- mean = zero
- variance = $\sigma^2_w$

$$x_t = (1 + \beta_1B + \beta_2B^2 + ... + \beta_qB^q)w_t = \phi_q(B)w_t$$

- $\phi_q$ is a polynomial of order q. 
- MA is stationary because it consists of a finite sum of stationary WN terms
- MA has time-invariant mean and time-invariant autocovariance

#### invertible MA(q) process

- if an MA(q) TS can be expressed as a stationary AR(infiniti) process of infinite order without an error term, it's invertible. 
- if the roots of $\phi_q(B)$ > 1 is absolute value, MA(q) is invertible. 

e.g. $$x_t = (1 - \beta B)w_t$$ is invertible:
$$w_t = (1-\beta B)^{-1}x_t$$ if $|\beta| < 1$ (condition for convergence)

```{r}
# to fit an MA(q) model to TS data, use
## arima(TSdata, order = c(0, 0, q))
x.ma <- arima(x, order = c(0, 0, 3))
x.ma
```

Example:

```{r}
x <- read.table("http://staff.elena.aut.ac.nz/Paul-Cowpertwait/ts/pounds_nz.dat", header = T)
x.ts <- ts(x, st = 1991, fr = 4)

# fit an MA(1) model to the data
x.ma <- arima(x.ts, order = c(0, 0, 1))
# check the variance
x.ma
# variance is 0.04172. more than the variance that we got from fitting a AR(1) model to the same data (with variance = 0.03125):
ar(x.ts)
# so AR(1) is a better model than MA(1) for this data.

acf(x.ma$res[-1])
```

---
### ARMA(p,q) models: AR(p) + MA(q) combined together: AutoRegressive Moving Average

- AR(p):
$$x_t = \alpha_1 x_{t-1} + \alpha_2 x_{t-2} + ... + \alpha_p x_{t-p} + w_t$$ 
- MA(q):
$$ x_t = w_t + \beta_1 w_{t-1} + ... + \beta_q w_{t - q} $$
- ARMA(p,q):
$$x_t = \alpha_1 x_{t-1} + \alpha_2 x_{t-2} + ... + \alpha_p x_{t-p} + w_t + \beta_1 w_{t-1} + ... + \beta_q w_{t - q} $$


$$\theta_p(B) x_t = \phi_q(B) w_t$$

- ARMA(p,q) TS is STATIONARY if roots of $\theta$ > 1 in absolute value
- ARMA(p,q) TS is INVERTIBLE if roots of $\phi$ > 1 in absolute value
- ARMA(p, 0) = AR(p)
- ARMA(0,q) = MA(q)
- ARMA(p,q) is better (parameter efficient) than either a single AR(p) or MA(q)

```{r}
set.seed(1)
# simulate an ARMA data with alpha = -0.6 and beta = 0.5
x <- arima.sim(n = 10000, list(ar = -0.6, ma = 0.5))
# fit an ARMA(1,1) model to the simulated data using c(p,0,q)
coef(arima(x, order = c(1, 0, 1)))
```

#### Now, let's compare AR(p), MA(q), and ARMA(p,q)

```{r}
x.ma <- arima(x.ts, order = c(0, 0, 1))
x.ar <- arima(x.ts, order = c(1, 0, 0))
x.arma <- arima(x.ts, order = c(1, 0, 1))
AIC(x.ma)
AIC(x.ar)
AIC(x.arma)
# the smaller the AIC or BIC, the better the fit => ARMA(1,1) provides the best fit
x.arma

# correlogram of residuals of fitted ARMA(1,1) model
acf(resid(x.arma))
```

#### example: electricity production series

```{r}
CBE <- read.table("http://staff.elena.aut.ac.nz/Paul-Cowpertwait/ts/cbe.dat", header = T)
Elec.ts <- ts(CBE[, 3], start = 1958, freq = 12)
Time <- 1:length(Elec.ts)
Imth <- cycle(Elec.ts)
Elec.lm <- lm(log(Elec.ts) ~ Time + I(Time^2) + factor(Imth))
acf(resid(Elec.lm))

# find the best p and q for ARMA(p,q)
best.order <- c(0, 0, 0)
best.aic <- Inf
for (i in 0:2) for (j in 0:2) {
  fit.aic <- AIC(arima(resid(Elec.lm), order = c(i, 0,j)))
  if (fit.aic < best.aic) {
    best.order <- c(i, 0, j)
    best.arma <- arima(resid(Elec.lm), order = best.order)
    best.aic <- fit.aic
  }
}
# what's the best order found:
best.order
# correlogram
acf(resid(best.arma))
```

---

## prediction

something

```{r}
new.time <- seq(length(Elec.ts), length = 36)
new.data <- data.frame(Time = new.time, Imth = rep(1:12,3))
# predict using the fitted regression model of class lm 
predict.lm <- predict(Elec.lm, new.data)
# predict using the fitted ARMA model, number of timesteps ahead to predict; here = 36 ~ predict for the next three years
predict.arma <- predict(best.arma, n.ahead = 36)
# convert the predicted values to TS object
elec.pred <- ts(exp(predict.lm + predict.arma$pred), start = 1991, freq = 12)
ts.plot(cbind(Elec.ts, elec.pred), lty = 1:2)
```

---
### how to tell if a correlogram is of AR(1), MA(1) or WN

AR(1): smooth decay
AR(1) with negative correlation: smooth decay, alternating positive and negative
MA(1): lag 0 = 1, lag 1 = beta_1
WN: lag 0 = 1, the rest are almost zero. 

## Regression

#### Time series regression vs. regular regression? 

TS regression is different because its residual forms a TS, and is serially correlated. (if this correlation is positive => estimated s.e. of parameter estimates are less than their true values => erroneously low p-values)
how to fix this? and get a proper estimated s.e.? use GLS (generalized least squares) to calculate estimated s.e.

To do regression, we need to make the TS stationary, if it's non-stationary.
e.g. if TS is $x_t = \alpha_0 + \alpha_1 t + z_t$, which is non-stationary, because it depends on t, then,

$$\Delta x_t = x_t - x_{t-1} = \alpha_1 + z_t - z_{t-1}$$ 

i.e. first order differencing, removes the trend, and makes it stationary. (assuming that $z_t$ is stationary error series)

If the underlying trend is a polynomial of order m, then we need to do $m^{th}$-order differencing to remove its trend.

```{r}
set.seed(1)
# Gaussian WN
z <- w <- rnorm(100, sd = 20)
# noise = AR(1) with alpha 0.8 and WN 
for (t in 2:100) z[t] <- 0.8 * z[t - 1] + w[t]
Time <- 1:100
# TS with a straight line trend and AR(1) residual error noise
x <- 50 + 3 * Time + z
plot(x, xlab = "time", type = "l")

# fit a linear regression model
x.lm <- lm(x ~ Time)
# look at the coefficients of the estimated linear regression model
coef(x.lm)
# calculate s.e. (standard errors): will be under-estimated because of serial correlation
sqrt(diag(vcov(x.lm)))
# summary() will give erroneous info, because of the under-estimated s.e. => incorrect p-values
summary(x.lm)

# check diagnostic plots
acf(residuals(x.lm))
pacf(residuals(x.lm))
```

another example:

```{r}
Global <- scan("http://staff.elena.aut.ac.nz/Paul-Cowpertwait/ts/global.dat")
Global.ts <- ts(Global, st = c(1856, 1), end = c(2005, 12), fr = 12)
temp <- window(Global.ts, start = 1970)
temp.lm <- lm(temp ~ time(temp))
coef(temp.lm)
confint(temp.lm)
acf(resid(lm(temp ~ time(temp))))
```

---

### GLS (generalized least squares) 

- GLS is a fitting procedure
- to estimate the s.e. of the regression model parameters
- better than the OLS we used before

```{r}
library(nlme)
x.gls <- gls(x ~ Time, cor = corAR1(0.8))
summary(x.gls)

temp.gls <- gls(temp ~ time(temp), cor = corAR1(0.7))
summary(temp.gls)
# confidence interval 
confint(temp.gls)

# note that the confidence interval does not include zero => estimates are statistically significant => trend is significant => statistically significant increasing trend in global temperatures over the 1970-2005 period 

```

### Linear models with seasonality 

treat the seasonal term as a factor! (e.g. 12 factors for monthly data TS) and do GLS or OLS like before.

```{r}
Seas <- cycle(temp)
Time <- time(temp)
# factors for seasonality. Use 0 in model to make sure the model won't have an intercept, otherwise one of the seasonal factors will be dropped and an intercept estimate will be in the output model
temp.lm <- lm(temp ~ 0 + Time + factor(Seas))
summary(temp.lm)
coef(temp.lm)

# predict two years ahead
new.t <- seq(2006, len = 2 * 12, by = 1/12)
new.dat <- data.frame(Time = new.t, Seas = rep(1:12, 2))
predict(temp.lm, new.dat)[1:24]
```

### Regression using Harmonic seasonal models instead of simple linear regression models

instead of using one parameter per season, smooth over the seasons => more parameter-efficient

A sine wave with frequency f (cycles per sampling interval), amplitude A, and phase shift $\phi$ can be expressed as:

$$ A \sin(2\pi f t + \phi) = \alpha_s \sin(2\pi f t) + \alpha_c \cos(2\pi f t) $$

For a time series {x t } with s seasons there are [s/2] possible cycles. e.g. for monthly data, s = 12, there are 6 possible cycles.

The model will be:

$$ x_t = m_t + \sum_{i = 1}^{[s/2]} (s_i \sin(2\pi i t /s) + c_i \cos(2\pi i t/s) + z_t $$

where:

- $m_t$ is trend
- $s_i$ and $c_i$ unknown parameters


```{r}
TIME <- seq(1, 12, len = 1000)
plot(TIME, sin(2 * pi * TIME/12), type = "l")
plot(TIME, sin(2 * pi * TIME/12) + 
       0.2 * sin(2 * pi * 2 * TIME/12) + 
       0.1 * sin(2 * pi * 4 * TIME/12) + 
       0.1 * cos(2 * pi * 4 * TIME/12), 
     type = "l")

set.seed(1)
TIME <- 1:(10 * 12)
w <- rnorm(10 * 12, sd = 0.5)
Trend <- 0.1 + 0.005 * TIME + 0.001 * TIME^2
Seasonal <- sin(2*pi*TIME/12) +
  0.2*sin(2*pi*2*TIME/12) +
  0.1*sin(2*pi*4*TIME/12) + 
  0.1*cos(2*pi*4*TIME/12)
x <- Trend + Seasonal + w
plot(x, type = "l")

SIN <- COS <- matrix(nr = length(TIME), nc = 6)
for (i in 1:6) {
  COS[, i] <- cos(2 * pi * i * TIME/12)
  SIN[, i] <- sin(2 * pi * i * TIME/12)
}

x.lm1 <- lm(x ~ TIME + I(TIME^2) + 
              COS[, 1] + SIN[, 1] + 
              COS[, 2] + SIN[, 2] + 
              COS[, 3] + SIN[, 3] + 
              COS[, 4] + SIN[, 4] + 
              COS[, 5] + SIN[, 5] + 
              COS[, 6] + SIN[, 6])
coef(x.lm1)/sqrt(diag(vcov(x.lm1)))

x.lm2 <- lm(x ~ I(TIME^2) + SIN[, 1] + SIN[, 2])

coef(x.lm2)/sqrt(diag(vcov(x.lm2)))
coef(x.lm2)

AIC(x.lm1)
AIC(x.lm2)
AIC(lm(x ~ TIME +I(TIME^2) + 
         SIN[,1] +
         SIN[,2] +
         SIN[,4] +
         COS[,4]))

step(x.lm1)

```

#### Fit a harmonic model with a quadratic trend is fitted to the temperature series (1970â€“2005)

```{r}
SIN <- COS <- matrix(nr = length(temp), nc = 6)
for (i in 1:6) {
  COS[, i] <- cos(2 * pi * i * time(temp))
  SIN[, i] <- sin(2 * pi * i * time(temp))
}
TIME <- (time(temp) - mean(time(temp)))/sd(time(temp))
mean(time(temp))
sd(time(temp))
temp.lm1 <- lm(temp ~ TIME + I(TIME^2) + 
                 COS[,1] + SIN[,1] + COS[,2] + SIN[,2] + 
                 COS[,3] + SIN[,3] + COS[,4] + SIN[,4] +
                 COS[,5] + SIN[,5] + COS[,6] + SIN[,6])

coef(temp.lm1)/sqrt(diag(vcov(temp.lm1)))

temp.lm2 <- lm(temp ~ TIME + SIN[, 1] + SIN[, 2])
coef(temp.lm2)
AIC(temp.lm)
AIC(temp.lm1)
AIC(temp.lm2)

plot(time(temp), resid(temp.lm2), type = "l")
abline(0, 0, col = "red")
acf(resid(temp.lm2))
pacf(resid(temp.lm2))

res.ar <- ar(resid(temp.lm2), method = "mle")
res.ar$ar
sd(res.ar$res[-(1:2)])
acf(res.ar$res[-(1:2)])

```