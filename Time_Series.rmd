---
title: "Time Series"
author: "Emerson Amirhosein  Azarbakht"
output: pdf_document
---

# Time Series (TS)

#### Used in
- control of inventory, based on demand trends
- airline's decision to buy airplanes bc of passenger trends and decision to increase/maintain market share
- climate change decisions based on temperature change trends
- business/sales forecasting
- everyday operational decisions
- long-term effects of proposed water management policies by simulating daily rainfall and sea state time series
- understanding fluctuations in monthly sales
- basis for signal processing in telecommunications <?>
- disease incidence tracking, yearly rates
- census analysis
- tracking monthly unemployment rate; as an economic indicator used by decision makers

#### Used to
- to understand the past, and predict the future
- forcasting (predicting inference, a subset of statistical inference). assumes that present trends continue. This assumption cannot be checked empirically, but, when we identify the likely causes for a trend, we can justify the forecasting(extrapolating it) for a few time-steps at least
- anomaly detection
- clustering
- classification (assigning a time series pattern to a specific category: e.g. gesture recognition of hand movements in sign language videos)
- query by content ~ Content-based image retrieval

#### Data: 
a variable measured sequentially in time, or at a fixed [sampling] interval 

#### serial dependence problem:
observations close together in time tend to be correlated (serially dependent)

TS tries to explain this correlation (serial dependence)
autocorrelation analysis examines this serial dependence <?>

### conditions (assumptions of TS)
- Stationary process:
  * if there's no systematic change in mean (no trend) AND
  * if there's no systematic change in variance, AND
  * if strictly periodic variations have been removed
    * i.e. the properties of one section of the data are much like those of any other section
    * often we have a non-stationary TS => we need to remove trend and seasonality, to get a stationary residual, which then can be modeled using a stationary stochastic process 
- Ergodic process ?

```{r}
plot(AirPassengers)
start(AirPassengers)
end(AirPassengers)
frequency(AirPassengers)
plot(AirPassengers)
summary(AirPassengers)
layout(1:2)
# takes an input matrix for the location of each plot in the graphics window
plot(aggregate(AirPassengers))
boxplot(AirPassengers ~ cycle(AirPassengers))
```

plotting shows _patterns_, and _features_ of the data + *outliers* and *erroneous* values 


### patterns
1. trend = a non-periodic systematic change in a TS: a long-term change in the 'mean'
    * can be modeled simply by a linear increase or decrease. (only if it's deterministic ~ it's non-stochastic)
    * stochastic trend: seems to change direction at unpredictable times rather than displaying a consistent pattern (e.g. like the air passenger series)
2. seasonal variation = a repeating pattern within a fixed period (e.g. each year)
3. cycles = a non-fixed-period cycle (without a fixed period). example: El-Nino

![El Nino picture](El-Nino.gif)

##### Monthly unemployment rate for a state
```{r}
# monthly unemployment rate for the US state of Maine from January 1996 until August 2006
Maine.month <- read.table("http://staff.elena.aut.ac.nz/Paul-Cowpertwait/ts/Maine.dat", header = TRUE)
# header TRUE means treat first row as column names
attach(Maine.month)
str(Maine.month)
head(Maine.month)
class(Maine.month)
# it's a data.frame, not a ts object. So, we need to convert it to ts
Maine.month.ts <- ts(unemploy, start = c(1996, 1), freq = 12)
Maine.month.ts
Maine.annual.ts <- aggregate(Maine.month.ts)/12
layout(1:2)
plot(Maine.month.ts)
plot(Maine.annual.ts)
Maine.Feb <- window(Maine.month.ts, start = c(1996,2), freq = TRUE)
Maine.Aug <- window(Maine.month.ts, start = c(1996,8), freq = TRUE)
Feb.ratio <- mean(Maine.Feb) / mean(Maine.month.ts)
Aug.ratio <- mean(Maine.Aug) / mean(Maine.month.ts)
```
---
#### Natoionwide unemployment rate
```{r}
# monthly unemployment rate for all of the United States from January 1996 until October 2006
US.month <- read.table("http://staff.elena.aut.ac.nz/Paul-Cowpertwait/ts/USunemp.dat", header = T)
attach(US.month)
US.month.ts <- ts(USun, start=c(1996,1), end=c(2006,10), freq = 12)
plot(US.month.ts, ylab = "unemployed (%)")
```
---
### Multiple TS 
```{r}
ChocolateBeerElectricity <- read.table("http://staff.elena.aut.ac.nz/Paul-Cowpertwait/ts/cbe.dat", header = TRUE)
class(ChocolateBeerElectricity)
str(ChocolateBeerElectricity)
Chocolate.ts <- ts(ChocolateBeerElectricity[,1], start = 1958, frequency = 12)
Beer.ts <- ts(ChocolateBeerElectricity[,2], start = 1958, frequency = 12)
Electricity.ts <- ts(ChocolateBeerElectricity[,3], start = 1958, frequency = 12)

plot(cbind(Chocolate.ts, Beer.ts, Electricity.ts))

# intersection of multiple TS
compareTwoTSChocolateBeer <- ts.intersect(Chocolate.ts, Beer.ts)
# bind time series which have a common frequency
?ts.intersect()
head(compareTwoTSChocolateBeer)

layout(1:2)
plot(compareTwoTSChocolateBeer[,1])
plot(compareTwoTSChocolateBeer[,2])
cor(Chocolate.ts, Beer.ts)
# correlation
```

#### Climate change
```{r}
# Global temperature anomalies from the monthly means over the period
GlobalTemperatures <- scan("http://staff.elena.aut.ac.nz/Paul-Cowpertwait/ts/global.dat")
# scan to read data into a vector or list from the console or file.
?scan
str(GlobalTemperatures)
GlobalTemperatures.ts <- ts(GlobalTemperatures, st = c(1856,1), end = c(2005,1), frequency = 12)
head(GlobalTemperatures.ts)
tail(GlobalTemperatures.ts)
Global.annual <- aggregate(GlobalTemperatures.ts, FUN = mean)
head(Global.annual)
Global.annual
plot(GlobalTemperatures.ts)
plot(Global.annual)  

#
GlobalTemperatures1970to2005 <- window(GlobalTemperatures.ts, start=c(1970, 1), end=c(2005, 12))
# subset a time window
?window
GlobalTemperatures1970to2005.time <- time(GlobalTemperatures1970to2005)
# create the vector of times at which a time series was sampled
?time
plot(GlobalTemperatures1970to2005)
abline(reg = lm(GlobalTemperatures1970to2005 ~ GlobalTemperatures1970to2005.time))
```

--- 

In statistics, usually, first thing, we compute the 'mean' and 'variance/standard deviation', which show 'location' and 'dispersion':

- mean ~ location
- variance ~ dispersion

### Transformations

Transform the data (e.g. log or square-root) when:

1. if variance appears to increase with the mean.
    * if standard deviation is directly proportional to the mean (a trend), LOG transform!
    * if variance changes through time, but without a trend, transformation won't help. use a model that accommodates variance change
2. if there's additive seasonal effect, i.e. size of seasonal effect increases with the mean, transform! to make seasonal effect constant. 
    * If there's multiplicative seasonal effect, i.e. size of seasonal effect proportional to the mean, LOG transform! to make the seasonal effect additive. (variance gets stable, but error term will still remain unstable)
3. if there are spikes in the data, (skewness), transform to normalize the data distribution.

> avoid transformations, except where the transformed has a direct physical interpretation

### Sample autocorrelation coefficients

- a series of quantities, that measure the correlation between observations at different distances apart
- show us which probability model to use for that data


- negative correlation? shows high values of x tend to go with low values of y
- zero correlation? shows two variables are independent

#### serial correlation coefficients (e.g. at lag 1), 
or autocorrelation coefficients

- measures correlation between successive observations
- serial correlation coefficients at lag k
- if you graph it, it's called 'correlogram'
- ac.f and correlogram is meaningful ONLY IF data is STATIONARY
- ac.f and correlogram is meaningless for non-stationary

### how to interpret a Correlogram

- if r_0 = 1, r_1 = large value, r_2, r_3 = diminishingly successively smaller values, r_k = almost zero
    then, TS: one observation 'above' the mean tends to be followed by more observations 'above' the mean, and
              one observation 'below' the mean tends to be followed by more observations 'below' the mean

- if r_0 = 1, r_1 = negative value, r_2 = positive value, r_3 = negative, ... (diminishingly successively smaller values)
    then, TS: one observation 'above' the mean tends to be followed by more observations 'below' the mean,
            aka alternating: successive observations on both sides of the overall mean

- if r_0...k are all positive and large values, TS is non-stationary, and correlogram is meaningless

- if r_0...k oscillate, TS is 'seasonal', e.g. sinusoidal. check at least 3 seasons worth of r_k


